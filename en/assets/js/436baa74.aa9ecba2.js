"use strict";(self.webpackChunkrill_flow_github_io=self.webpackChunkrill_flow_github_io||[]).push([[4447],{4772:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var r=t(5893),o=t(1151);const i={},s="Angular model",l={id:"best-practice/work-with-llms",title:"Angular model",description:"With the iterations of large model technology, more large model technologies are being applied in different business scenarios.Rill Flow is designed to perform long running tasks that fully meet the characteristics of large model calls.",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/best-practice/04-work-with-llms.md",sourceDirName:"best-practice",slug:"/best-practice/work-with-llms",permalink:"/en/docs/best-practice/work-with-llms",draft:!1,unlisted:!1,editUrl:"https://crowdin.com/project/rill-flow/en",tags:[],version:"current",sidebarPosition:4,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Server Access Service",permalink:"/en/docs/best-practice/work-with-servless"},next:{title:"\u5f00\u53d1\u8005\u6307\u5357",permalink:"/en/docs/category/\u5f00\u53d1\u8005\u6307\u5357"}},a={},d=[{value:"Exposure model via HTTP service",id:"exposure-model-via-http-service",level:2},{value:"Each model is deployed independently.",id:"each-model-is-deployed-independently",level:2},{value:"Distribution Storage",id:"distribution-storage",level:2},{value:"Serverless",id:"serverless",level:2}];function c(e){const n={blockquote:"blockquote",h1:"h1",h2:"h2",p:"p",...(0,o.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"angular-model",children:"Angular model"}),"\n",(0,r.jsx)(n.p,{children:"With the iterations of large model technology, more large model technologies are being applied in different business scenarios.Rill Flow is designed to perform long running tasks that fully meet the characteristics of large model calls."}),"\n",(0,r.jsx)(n.p,{children:"Rill Flow is able to take over large cloud models like ChatGPT, and can also handle large models for private deployment."}),"\n",(0,r.jsx)(n.h2,{id:"exposure-model-via-http-service",children:"Exposure model via HTTP service"}),"\n",(0,r.jsx)(n.p,{children:"Big models usually reveal only C++ or python interfaces if direct cross-language calls are not friendly to business landing."}),"\n",(0,r.jsx)(n.p,{children:"So we recommend using the HTTP protocol to encapsulate large model interfaces and FastAPI is a common HTTP framework for exposing large model interfaces."}),"\n",(0,r.jsx)(n.h2,{id:"each-model-is-deployed-independently",children:"Each model is deployed independently."}),"\n",(0,r.jsx)(n.p,{children:"Each large model usually requires a specific software and hardware operating environment, while models and fine-tuned versions are rapidly iterating as a result of rapid developments in large modelling areas, where multiple large models are deployed within the same operating environment significantly increases the complexity of operations and models."}),"\n",(0,r.jsx)(n.p,{children:"We therefore recommend the use of a large model of environmental deployment when operating independently based on Docker, K8S technology."}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"On the contrary, independent deployment of each large model also implies a need for greater deployment capacity."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"distribution-storage",children:"Distribution Storage"}),"\n",(0,r.jsx)(n.p,{children:"A large number of files may need to be pulled or produced when using a large type of image or video."}),"\n",(0,r.jsx)(n.p,{children:"File storage is not supported by the Context mechanism of Rill Flow; if there is a need to share files between different tasks, a distribution storage service is introduced, and storage addresses are passed through the context mechanism between task nodes."}),"\n",(0,r.jsx)(n.h2,{id:"serverless",children:"Serverless"}),"\n",(0,r.jsx)(n.p,{children:"Big models themselves are characterized by high deployment costs and low requests, and they can improve the efficiency of the GPU resources by matching large model services to Serverlessness."})]})}function h(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>l,a:()=>s});var r=t(7294);const o={},i=r.createContext(o);function s(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);