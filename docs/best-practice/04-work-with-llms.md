# 对接大模型
伴随着大模型技术的迭代，更多的大模型技术开始应用在不同业务场景中。Rill Flow被设计用来执行长时间运行的任务，完全符合大模型调用的特点。

Rill FLow能够对接ChatGPT这类云端大模型，同时也能对接私有部署的大模型服务。

## 通过HTTP服务暴露模型
大模型通常仅暴露C++或python接口，如果直接跨语言调用接口对于业务落地并不友好。

因此我们建议使用HTTP协议对大模型接口进行封装，FastAPI是一个常见的用于暴露大模型接口的HTTP框架。

## 每种模型独立部署运行
每种大模型通常需要特定的的软硬件运行时环境，同时，由于大模型领域的发展迅速，模型及调优版本正在快速迭代，如果在同一个运行时环境内部署多个大模型，会显著的提升业务和模型迭代复杂度。

因此，我们建议基于Docker、K8S技术，使用独立的运行时环境部署大模型。

> 相反的，为每种大模型做独立部署也意味着需要更强的部署调度能力。

## 分布式存储
当使用图片、视频相关的生成类大模型时，有可能需要拉取或生产大量文件。

Rill Flow的Context机制不支持文件存储，如果需要在不同任务间共享文件，需要引入分布式存储服务，不同任务节点间通过上下文机制传递存储地址。

## Serverless
大模型本身具有部署成本高、请求量低的特点，通过将大模型服务对接Serverless机制可以更好的提升GPU资源提升效率。
